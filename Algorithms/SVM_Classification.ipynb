{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from helper_methods.ipynb\n",
      "Importing Jupyter notebook from preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "import helper_methods as hm\n",
    "import preprocessing as pp\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier Implementation\n",
    "http://tullo.ch/articles/svm-py/  \n",
    "https://pythonprogramming.net/soft-margin-kernel-cvxopt-svm-machine-learning-tutorial/  \n",
    "https://xavierbourretsicotte.github.io/SVM_implementation.html  \n",
    "http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/kernels.pdf  \n",
    "https://courses.csail.mit.edu/6.867/wiki/images/a/a7/Qp-cvxopt.pdf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function imitates a Polynomial Kernel with the dimension d as input to the function\n",
    "def polynomial_kernel(x1, x2, degree = 3):\n",
    "    return (1 + np.dot(x1, x2)) ** degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x1, x2, gamma):\n",
    "    if gamma == 'auto':\n",
    "        gamma = 1 / len(x1)\n",
    "    return np.exp(gamma * (-np.linalg.norm(x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Support_Vector_Machine_Classifier:\n",
    "    def __init__(self, C = 1.0, kernel = 'rbf', degree = 3, gamma = 'auto'):\n",
    "        # https://stackoverflow.com/questions/60208/replacements-for-switch-statement-in-python\n",
    "        self._kernel = {\n",
    "                        'linear': linear_kernel,\n",
    "                        'poly': lambda x1, x2: polynomial_kernel(x1, x2, degree = degree),\n",
    "                        'rbf': lambda x1, x2: rbf_kernel(x1, x2, gamma = gamma),\n",
    "                    }.get(kernel, rbf_kernel) \n",
    "        # to force the float type\n",
    "        self._C = float(C)\n",
    "        self._degree = degree\n",
    "        self._gamma = gamma\n",
    "        print('C used:', C)\n",
    "        \n",
    "    # https://stackoverflow.com/questions/1547145/defining-private-module-functions-in-python\n",
    "    def _get_gram_matrix(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = self._kernel(X[i], X[j])\n",
    "        return K  \n",
    "        \n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        \n",
    "        K = self._get_gram_matrix(X_train)\n",
    "        \n",
    "        # We want to maximise wrt alpha, instead we minimise the negative of the expression wrt alpha\n",
    "        # We want to maximise Q(a)\n",
    "        # Q(a) = -(1/2)a(T)Pa + sum_i(a_i)\n",
    "        # Instead we minimise (-Q(a))\n",
    "        # Check SVM slide 14 and 20\n",
    "        \n",
    "        # P = sum i,j(yi * yj * K(xi, xj))\n",
    "        P = cvxopt.matrix(np.outer(Y_train, Y_train) * K)\n",
    "        # q = -1\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        \n",
    "        # Ax = b ====> sum_i(y_i * a_i) = 0\n",
    "        A = cvxopt.matrix(Y_train, (1, n_samples))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        \n",
    "        # Gx <= h =====> 0 <= a <= C\n",
    "        # Top half -----> -a <= 0\n",
    "        top_half = np.eye(n_samples) * -1\n",
    "        # Bottom half -----> a <= C\n",
    "        bot_half = np.eye(n_samples)\n",
    "        G = cvxopt.matrix(np.vstack((top_half, bot_half)))\n",
    "        \n",
    "        top_half = np.zeros(n_samples)\n",
    "        bot_half = np.ones(n_samples) * self._C\n",
    "        h = cvxopt.matrix(np.hstack((top_half, bot_half)))\n",
    "        \n",
    "        # Silent cvxopt solver - https://stackoverflow.com/a/27184422/6001624\n",
    "        cvxopt.solvers.options['show_progress'] = True\n",
    "        \n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers (the alphas)\n",
    "        alphas = np.ravel(solution['x'])\n",
    "\n",
    "        # Support vectors have non zero lagrange multipliers (Check slide 22 SVM)\n",
    "        is_sv = alphas > 1e-5\n",
    "        sv_indices = np.arange(len(alphas))[is_sv]\n",
    "        self._alphas = alphas[is_sv]\n",
    "        \n",
    "        # the x part of the SVs\n",
    "        self._support_vectors = X_train[is_sv]\n",
    "        # the y part of the SVs\n",
    "        self._support_vector_labels = Y_train[is_sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self._alphas), n_samples))\n",
    "\n",
    "        # Slide 12: http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/kernels.pdf\n",
    "        # Intercept -----> b = mean(y_k - sum_i(a_i * y_i * x_i).x_k) for all k st 0 < a_k < C\n",
    "        #                    = mean(y_k - sum(a_i * y_i * K(x_i, x_k))) for all k st 0 < a_k < C\n",
    "        self._b = 0\n",
    "        for i in range(len(self._alphas)):\n",
    "            self._b += self._support_vector_labels[i]\n",
    "            self._b -= np.sum(self._alphas * self._support_vector_labels * K[sv_indices[i],is_sv]) # use is_sv as only those k st 0 < a_k < C\n",
    "        self._b /= len(self._alphas)\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y_predict = np.ones(len(X)) * self._b\n",
    "        for i in range(len(X)):\n",
    "            for alpha, sv_y, sv in zip(self._alphas, self._support_vector_labels, self._support_vectors):\n",
    "                y_predict[i] += alpha * sv_y * self._kernel(X[i], sv)\n",
    "        return np.sign(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Optimal Parameters\n",
    "https://stats.stackexchange.com/questions/43943/which-search-range-for-determining-svm-optimal-c-and-gamma-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameters_accuracy(accuracies):\n",
    "    print('#Features \\t C \\t Gamma \\t Accuracy')\n",
    "    for i in range(len(accuracies)):\n",
    "        print(accuracies[i][0], '\\t\\t', accuracies[i][1], '\\t\\t', accuracies[i][2], '\\t\\t', accuracies[i][3])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_values(max_features, C_list, gamma_list, kernel = 'rbf', num_splits = 10, symbol_name = 'AAPL', use_implementation = True):\n",
    "    accuracies = list()\n",
    "    for num_features in range(1, max_features + 1, 1):\n",
    "        print('Features:', num_features)\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = hm.prepare_data(num_features, symbol_name, is_binary_ouput=True)\n",
    "        X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "        \n",
    "        for C in C_list: # [1, 10, 100, 500, 1000, 5000, 10000]:\n",
    "            for gamma in gamma_list: # [1, 0.1, 0.05, 0.01, 0.005, 0.001]:\n",
    "                print('(C, gamma) ------------------------> (' + str(C) + ', ' + str(gamma) + ')')\n",
    "                skl_svm_tscv = svm.SVC(kernel=kernel, C=C, gamma=gamma)\n",
    "#                 svm_tscv = Support_Vector_Machine_Classifier(C = C, kernel = kernel, gamma = gamma)\n",
    "                if use_implementation:\n",
    "                    accuracy = hm.timeSeriesCV(X_train, Y_train, num_splits, skl_svm_tscv, is_classification=True)\n",
    "                else:\n",
    "                    accuracy = hm.rolling_cross_validation(X_train, Y_train, num_splits, svm_tscv, is_classification=True)\n",
    "                accuracies.append([num_features, C, gamma, accuracy])\n",
    "    \n",
    "    print_parameters_accuracy(accuracies)\n",
    "    \n",
    "    # Sorting the accuracies\n",
    "    accuracies.sort(reverse=True, key=lambda x: x[len(accuracies) - 1])\n",
    "    print_parameters_accuracy(accuracies)\n",
    "    \n",
    "    return accuracies[0][0], accuracies[0][1], accuracies[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ready(symbol_name, max_features, C_list, gamma_list, kernel):\n",
    "    start_time = time.time()\n",
    "#     num_features, C, gamma = find_optimal_values(max_features, C_list, gamma_list, kernel, \n",
    "#                                                  num_splits=10, symbol_name = symbol_name)\n",
    "    num_features, C, gamma = 3, 100000000, 0.01\n",
    "    end_time = time.time()\n",
    "    print('Time taken for Cross Validation:', end_time - start_time)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = hm.prepare_data(num_features)\n",
    "    X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "    return X_train, X_test, Y_train, Y_test, C, gamma    \n",
    "    \n",
    "#     X_train, X_test, Y_train, Y_test = hm.prepare_data(2)\n",
    "#     X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "#     return X_train, X_test, Y_train, Y_test, 1.0, 0.25   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SkLearn SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C = 1.0, kernel = 'rbf'):\n",
    "    print('SKLEARN INBUILT SVM')\n",
    "    clf = svm.SVC(kernel = kernel, C = C, gamma=gamma)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print('In-Built SVM (Accuracy) score -- kernel =', kernel, '--', clf.score(X_test, Y_test), '\\n')\n",
    "    print('Number of SVs =', len(clf.support_vectors_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predicting using Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implemented_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C = 1.0, kernel = 'rbf'):\n",
    "    my_SVM = Support_Vector_Machine_Classifier(C = C, kernel = kernel, gamma = gamma)\n",
    "    print('Implemenation Fit Starting...')\n",
    "    my_SVM.fit(X_train, Y_train)\n",
    "    \n",
    "    print('IMPLEMENTATION') \n",
    "    Y_pred = my_SVM.predict(X_test)\n",
    "    hm.accuracy_metrics(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(X_train, X_test, Y_train, Y_test, gamma, C = 1.0, kernel = 'rbf'):\n",
    "    print('C:', C, '\\t gamma:', gamma)\n",
    "    sklearn_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C, kernel = kernel)\n",
    "    implemented_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C, kernel)\n",
    "    \n",
    "# X_train, X_test, Y_train, Y_test = hm.prepare_data(2)\n",
    "# X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "# forecast(X_train, X_test, Y_train, Y_test, 0.25, 1, 'rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://m.svms.org/regression/TaCa01.pdf  \n",
    "Sensitivity of SVMs to parameters  \n",
    "Application of support vector machines in Financial time series forecasting  \n",
    "Francis E.H. Tay ∗, Lijuan Cao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SVM(symbol_name):\n",
    "    max_features = 10\n",
    "    kernel = 'rbf'\n",
    "    C_list = [1, 10, 33, 55, 78, 100]\n",
    "    gamma_list = [1/100, 1/75, 1/50, 1/25, 1/10, 1]\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, C, gamma = get_data_ready(symbol_name, max_features, C_list, gamma_list, kernel)\n",
    "    forecast(X_train, X_test, Y_train, Y_test, gamma, C, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for Cross Validation: 9.5367431640625e-07\n",
      "C: 100000000 \t gamma: 0.01\n",
      "SKLEARN INBUILT SVM\n",
      "In-Built SVM (Accuracy) score -- kernel = rbf -- 0.5160550458715596 \n",
      "\n",
      "Number of SVs = 2435\n",
      "C used: 100000000\n",
      "Implemenation Fit Starting...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  3.9285e+13 -1.7176e+16  2e+16  2e-06  3e-06\n",
      " 1: -2.6451e+10 -3.8674e+14  4e+14  1e-05  3e-06\n",
      " 2: -1.9497e+11 -4.8278e+12  5e+12  5e-06  3e-06\n",
      " 3: -2.0943e+11 -8.5594e+11  6e+11  4e-06  3e-06\n",
      " 4: -3.0027e+11 -4.4043e+11  1e+11  5e-06  4e-06\n",
      " 5: -3.7309e+11 -3.9954e+11  3e+10  2e-06  5e-06\n",
      " 6: -3.7682e+11 -3.9652e+11  2e+10  5e-06  5e-06\n",
      " 7: -3.7913e+11 -3.9449e+11  2e+10  7e-06  5e-06\n",
      " 8: -3.8101e+11 -3.9271e+11  1e+10  1e-06  5e-06\n",
      " 9: -3.8234e+11 -3.9136e+11  9e+09  2e-05  5e-06\n",
      "10: -3.8291e+11 -3.9081e+11  8e+09  6e-06  5e-06\n",
      "11: -3.8374e+11 -3.8998e+11  6e+09  1e-05  5e-06\n",
      "12: -3.8452e+11 -3.8921e+11  5e+09  5e-06  5e-06\n",
      "13: -3.8503e+11 -3.8872e+11  4e+09  3e-06  5e-06\n",
      "14: -3.8559e+11 -3.8817e+11  3e+09  1e-05  5e-06\n",
      "15: -3.8596e+11 -3.8778e+11  2e+09  2e-05  5e-06\n",
      "16: -3.8627e+11 -3.8747e+11  1e+09  6e-06  5e-06\n",
      "17: -3.8652e+11 -3.8720e+11  7e+08  8e-06  6e-06\n",
      "18: -3.8668e+11 -3.8704e+11  4e+08  1e-05  6e-06\n",
      "19: -3.8681e+11 -3.8690e+11  1e+08  3e-05  7e-06\n",
      "20: -3.8684e+11 -3.8687e+11  2e+07  2e-05  7e-06\n",
      "21: -3.8685e+11 -3.8686e+11  3e+06  2e-16  6e-06\n",
      "22: -3.8685e+11 -3.8685e+11  3e+04  2e-05  7e-06\n",
      "23: -3.8685e+11 -3.8685e+11  3e+02  1e-05  7e-06\n",
      "24: -3.8685e+11 -3.8685e+11  3e+00  5e-06  7e-06\n",
      "25: -3.8685e+11 -3.8685e+11  3e-02  2e-05  7e-06\n",
      "26: -3.8685e+11 -3.8685e+11  3e-04  3e-05  7e-06\n",
      "27: -3.8685e+11 -3.8685e+11  3e-06  4e-05  7e-06\n",
      "28: -3.8685e+11 -3.8685e+11  3e-08  3e-06  2e-06\n",
      "29: -3.8685e+11 -3.8685e+11  3e-10  1e-06  6e-07\n",
      "30: -3.8685e+11 -3.8685e+11  3e-12  5e-07  5e-07\n",
      "31: -3.8685e+11 -3.8685e+11  3e-14  5e-07  5e-07\n",
      "32: -3.8685e+11 -3.8685e+11  3e-16  2e-16  5e-07\n",
      "33: -3.8685e+11 -3.8685e+11  3e-18  2e-16  5e-07\n",
      "34: -3.8685e+11 -3.8685e+11  3e-20  2e-16  5e-07\n",
      "35: -3.8685e+11 -3.8685e+11  3e-22  1e-16  5e-07\n",
      "36: -3.8685e+11 -3.8685e+11  3e-24  2e-16  5e-07\n",
      "37: -3.8685e+11 -3.8685e+11  3e-26  2e-16  5e-07\n",
      "38: -3.8685e+11 -3.8685e+11  3e-28  1e-06  6e-07\n",
      "39: -3.8685e+11 -3.8685e+11  3e-30  2e-16  5e-07\n",
      "40: -3.8685e+11 -3.8685e+11  3e-32  5e-07  5e-07\n",
      "41: -3.8685e+11 -3.8685e+11  3e-34  1e-16  5e-07\n",
      "42: -3.8685e+11 -3.8685e+11  3e-36  5e-07  5e-07\n",
      "43: -3.8685e+11 -3.8685e+11  3e-38  1e-06  5e-07\n",
      "44: -3.8685e+11 -3.8685e+11  3e-40  1e-06  5e-07\n",
      "45: -3.8685e+11 -3.8685e+11  3e-42  1e-06  5e-07\n",
      "46: -3.8685e+11 -3.8685e+11  3e-44  5e-07  5e-07\n",
      "47: -3.8685e+11 -3.8685e+11  3e-46  5e-07  5e-07\n",
      "48: -3.8685e+11 -3.8685e+11  3e-48  1e-16  5e-07\n",
      "49: -3.8685e+11 -3.8685e+11  3e-50  5e-07  5e-07\n",
      "50: -3.8685e+11 -3.8685e+11  3e-52  2e-16  5e-07\n",
      "51: -3.8685e+11 -3.8685e+11  3e-54  2e-16  5e-07\n",
      "52: -3.8685e+11 -3.8685e+11  3e-56  2e-16  6e-07\n",
      "53: -3.8685e+11 -3.8685e+11  3e-58  5e-07  5e-07\n",
      "54: -3.8685e+11 -3.8685e+11  3e-60  1e-06  6e-07\n",
      "55: -3.8685e+11 -3.8685e+11  3e-62  1e-06  6e-07\n",
      "56: -3.8685e+11 -3.8685e+11  3e-64  1e-16  6e-07\n",
      "57: -3.8685e+11 -3.8685e+11  3e-66  5e-07  5e-07\n",
      "58: -3.8685e+11 -3.8685e+11  3e-68  5e-07  5e-07\n",
      "59: -3.8685e+11 -3.8685e+11  3e-70  5e-07  5e-07\n",
      "60: -3.8685e+11 -3.8685e+11  3e-72  5e-07  5e-07\n",
      "61: -3.8685e+11 -3.8685e+11  3e-74  2e-16  5e-07\n",
      "62: -3.8685e+11 -3.8685e+11  3e-76  1e-16  5e-07\n",
      "63: -3.8685e+11 -3.8685e+11  3e-78  2e-16  5e-07\n",
      "64: -3.8685e+11 -3.8685e+11  3e-80  5e-07  5e-07\n",
      "65: -3.8685e+11 -3.8685e+11  3e-82  1e-06  5e-07\n",
      "66: -3.8685e+11 -3.8685e+11  3e-84  2e-06  5e-07\n",
      "67: -3.8685e+11 -3.8685e+11  3e-86  1e-06  5e-07\n",
      "68: -3.8685e+11 -3.8685e+11  3e-88  1e-16  5e-07\n",
      "69: -3.8685e+11 -3.8685e+11  3e-90  5e-07  5e-07\n",
      "70: -3.8685e+11 -3.8685e+11  3e-92  5e-07  6e-07\n",
      "71: -3.8685e+11 -3.8685e+11  3e-94  5e-07  5e-07\n",
      "72: -3.8685e+11 -3.8685e+11  3e-96  1e-06  5e-07\n",
      "73: -3.8685e+11 -3.8685e+11  3e-98  1e-06  5e-07\n",
      "74: -3.8685e+11 -3.8685e+11  3e-100  5e-07  5e-07\n",
      "75: -3.8685e+11 -3.8685e+11  3e-102  5e-07  5e-07\n",
      "76: -3.8685e+11 -3.8685e+11  3e-104  5e-07  5e-07\n",
      "77: -3.8685e+11 -3.8685e+11  3e-106  5e-07  5e-07\n",
      "78: -3.8685e+11 -3.8685e+11  3e-108  2e-16  5e-07\n",
      "79: -3.8685e+11 -3.8685e+11  3e-110  2e-16  5e-07\n",
      "80: -3.8685e+11 -3.8685e+11  3e-112  5e-07  5e-07\n",
      "81: -3.8685e+11 -3.8685e+11  3e-114  1e-06  6e-07\n",
      "82: -3.8685e+11 -3.8685e+11  3e-116  2e-16  5e-07\n",
      "83: -3.8685e+11 -3.8685e+11  3e-118  2e-16  5e-07\n",
      "84: -3.8685e+11 -3.8685e+11  3e-120  1e-06  5e-07\n",
      "85: -3.8685e+11 -3.8685e+11  3e-122  1e-06  5e-07\n",
      "86: -3.8685e+11 -3.8685e+11  3e-124  2e-16  5e-07\n",
      "87: -3.8685e+11 -3.8685e+11  3e-126  2e-16  6e-07\n",
      "88: -3.8685e+11 -3.8685e+11  3e-128  2e-16  5e-07\n",
      "89: -3.8685e+11 -3.8685e+11  3e-130  5e-07  6e-07\n",
      "90: -3.8685e+11 -3.8685e+11  3e-132  1e-06  5e-07\n",
      "91: -3.8685e+11 -3.8685e+11  3e-134  1e-16  5e-07\n",
      "92: -3.8685e+11 -3.8685e+11  3e-136  5e-07  5e-07\n",
      "93: -3.8685e+11 -3.8685e+11  3e-138  2e-16  5e-07\n",
      "94: -3.8685e+11 -3.8685e+11  3e-140  1e-06  5e-07\n",
      "95: -3.8685e+11 -3.8685e+11  3e-142  2e-16  5e-07\n",
      "96: -3.8685e+11 -3.8685e+11  3e-144  5e-07  5e-07\n",
      "97: -3.8685e+11 -3.8685e+11  3e-146  5e-07  5e-07\n",
      "98: -3.8685e+11 -3.8685e+11  3e-148  2e-16  5e-07\n",
      "99: -3.8685e+11 -3.8685e+11  3e-150  5e-07  5e-07\n",
      "100: -3.8685e+11 -3.8685e+11  3e-152  1e-16  6e-07\n",
      "Terminated (maximum number of iterations reached).\n",
      "3881 support vectors out of 3923 points\n",
      "IMPLEMENTATION\n",
      "Accuracy: 0.5061162079510704\n",
      "Matthews Correlation Coefficient: 0.014006785255109561\n",
      "Confustion Matrix\n",
      "[[371 275]\n",
      " [371 291]]\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.50      0.57      0.53       646\n",
      "        1.0       0.51      0.44      0.47       662\n",
      "\n",
      "avg / total       0.51      0.51      0.50      1308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_SVM(symbol_name = 'AAPL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
