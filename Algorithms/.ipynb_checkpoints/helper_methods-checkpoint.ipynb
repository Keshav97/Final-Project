{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preprocessing as pp\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fumction calculates and returns the Mean Squared Error (MSE) for a model\n",
    "def mean_squared_error(Y_true, Y_pred):\n",
    "    # number of test samples\n",
    "    M = len(Y_true)\n",
    "    total_error = 0\n",
    "    for i in range(M):\n",
    "        total_error += (Y_true[i] - Y_pred[i]) ** 2\n",
    "    mse = total_error / M\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits the data into train and test sets based on the split ratio supplied and \n",
    "# returns the corresponding X and Y values for the train and test data\n",
    "def train_test_split(df, train_ratio = 0.75):\n",
    "    split_index = int(train_ratio * len(df.index))\n",
    "    train_data = df.iloc[0:split_index, :]\n",
    "    test_data = df.iloc[split_index:, :]\n",
    "    \n",
    "    X_train, Y_train = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
    "    X_test, Y_test = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits the input data into corresponding X(features) and Y(output) values\n",
    "# The Y(output) values are assumed to be in the last column of the input data\n",
    "def get_X_Y(data):\n",
    "    X_data = list()\n",
    "    Y_data = list()\n",
    "    for i in data:\n",
    "        X_data.append(i[:-1])\n",
    "        Y_data.append(i[-1])\n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_binary_indicators(df):\n",
    "    output_column_index = len(df.columns) - 1\n",
    "    for i in range(len(df)):\n",
    "        df.iloc[i, output_column_index] = 1 if df.iloc[i, output_column_index] > 0 else 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(num_features, symbol_name = 'AAPL', is_binary_ouput = True):\n",
    "    \"\"\"\n",
    "    INPUT: This function taken in the Symbol of the stock data to be studied and eventually predicted \n",
    "    \n",
    "    It prepares the data for training by fetching the data for the mentioned stock, calculating the daily returns of the stock, \n",
    "    and adding corresponding features for each datapoint using n previous returns. At the end, if the output is wanted to be binary\n",
    "    i.e. Stock going up or down, positive returns are converted to 1 else converted to 0\n",
    "    It further splits the built dataframe into training and test sets\n",
    "     \n",
    "    RETURN: It returns X and Y values for the train and test sets ready to start work upon\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pp.read_file(symbol_name)\n",
    "    df = pp.data_daily_returns(df)\n",
    "    \n",
    "    # number of datapoints in the data frame\n",
    "    M = len(df.index)\n",
    "    \n",
    "    # creating columns for t = (t0 - n) return to t = t0 return\n",
    "    col_names = [str(i) for i in range(num_features, -1, -1)]\n",
    "    \n",
    "    # creating dataframe to store the RETURNS data points with correpsonding features\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for i in range(num_features + 1):\n",
    "        # gets the indices of the datapoints to build the features corresponding to RETURNS at t = t0 - (num_features - i)\n",
    "        indices_to_fetch = df.index[i:(M - num_features + i)]\n",
    "        temp = df.loc[indices_to_fetch, 'RETURNS']\n",
    "        temp.index = df.index[num_features:]\n",
    "        data[str(num_features - i)] = temp\n",
    "\n",
    "    if is_binary_ouput:    \n",
    "        data = output_to_binary_indicators(data)    \n",
    "    \n",
    "    return train_test_split(data, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = prepare_data(5)\n",
    "X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn TimeSeriesSplit\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cross_validation(X, Y, num_splits, algorithm, parameters, is_classification = True):\n",
    "    print('Inbuilt Rolling Cross Validation')\n",
    "    if num_splits <= 0 or num_splits >= len(X):\n",
    "        num_splits = len(X) - 1\n",
    "    print('Parameters ------------------------>', parameters)\n",
    "    \n",
    "    accuracies = []\n",
    "    tscv = TimeSeriesSplit(n_splits = num_splits)\n",
    "    \n",
    "    for train, test in tscv.split(X):\n",
    "#         print(\"Test: %s Train: %s\" % (test, train))\n",
    "        X_train, Y_train = X[train], Y[train]\n",
    "        X_test, Y_test = X[test], Y[test]\n",
    "        \n",
    "        Y_pred = algorithm(X_train, Y_train, X_test, parameters[0])\n",
    "        \n",
    "        if is_classification:\n",
    "            accuracies.append(accuracy_score(Y_test, Y_pred))\n",
    "        else:\n",
    "            accuracies.append(r2_score(Y_test, Y_pred))\n",
    "    \n",
    "    mean_accuracy = np.array(accuracies).mean()\n",
    "    print('Accuracy:', mean_accuracy)\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation on a Rolling Basis - Implementation\n",
    "\n",
    "we train our model on a small segment of the time series from the beginning until some  𝑡 , make predictions for the next  𝑡+𝑛 steps, and calculate an error. Then, we expand our training sample to  𝑡+𝑛  value, make predictions from  𝑡+𝑛  until  𝑡+2∗𝑛 , and continue moving our test segment of the time series until we hit the last available observation. As a result, we have as many folds as  𝑛  will fit between the initial training sample and the last observation.  \n",
    "\n",
    "http://francescopochetti.com/pythonic-cross-validation-time-series-pandas-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSeriesCV(X_train, Y_train, num_splits, algorithm, parameters, is_classification = True):\n",
    "    print('Implemented Rolling Cross Validation')\n",
    "    if num_splits <= 0 or num_splits >= len(X_train):\n",
    "        num_splits = 10\n",
    "    \n",
    "    print('Parameters ------------------------>', parameters)\n",
    "#     print('Number of folds:', num_splits)\n",
    "#     print('Size train set:', X_train.shape)\n",
    "    \n",
    "    k = int(np.floor(float(X_train.shape[0]) / num_splits))\n",
    "#     print('Size of each fold:', k)\n",
    "    \n",
    "    accuracies = np.zeros(num_splits - 1)\n",
    "    \n",
    "    for i in range(2, num_splits + 1):\n",
    "#         print('')\n",
    "        \n",
    "        split = float(i - 1)/i\n",
    "#         print('Splitting the first ' + str(i) + ' chunks at ' + str(i - 1) + '/' + str(i))\n",
    "        \n",
    "        X = X_train[:(k * i)]\n",
    "        Y = Y_train[:(k * i)]\n",
    "#         print('Size of train + test:', X.shape)\n",
    "        \n",
    "        index = int(np.floor(X.shape[0] * split))\n",
    "        \n",
    "        X_trainFolds = X[:index]\n",
    "        Y_trainFolds = Y[:index]\n",
    "        \n",
    "        X_testFolds = X[index:]\n",
    "        Y_testFolds = Y[index:]\n",
    "        \n",
    "        Y_pred = algorithm(X_trainFolds, Y_trainFolds, X_testFolds, parameters[0])\n",
    "        \n",
    "        if is_classification:\n",
    "            accuracies[i - 2] = accuracy_score(Y_testFolds, Y_pred)\n",
    "        else:\n",
    "            accuracies[i - 2] = r2_score(Y_testFolds, Y_pred)\n",
    "#         print('Accuracy of fold ' + str(i) + ':' + str(accuracies[i - 2]))\n",
    "    \n",
    "    mean_accuracy = accuracies.mean()\n",
    "    print('Accuracy:', mean_accuracy)\n",
    "    return mean_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
