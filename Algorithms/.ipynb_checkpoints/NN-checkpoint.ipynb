{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from helper_methods.ipynb\n",
      "Importing Jupyter notebook from preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "import helper_methods as hm\n",
    "import preprocessing as pp\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    \n",
    "    def __init__(self, n_hidden = 2, learning_rate = 0.01, epochs = 10000):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    # ACTIVATION - Sigmoid Function\n",
    "    def sig(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Derivative of Sigmoid Function\n",
    "    def derivativeSig(self, z):\n",
    "        return self.sig(z) * (1 - self.sig(z))\n",
    "    \n",
    "    def initialise_weights_biases(self, X_train, Y_train):\n",
    "        # Number of input neurons equal the number of input features\n",
    "        self.n_input = len(X_train[0])\n",
    "        \n",
    "        # Need only single output neuron to decide up or down\n",
    "        self.n_output = 1\n",
    "        \n",
    "        # Initialising weights and biases to random number between -1 and 1\n",
    "        self.weights = {\n",
    "            'h1': 2 * np.random.random((self.n_input, self.n_hidden)) - 1,\n",
    "            'out': 2 * np.random.random((self.n_hidden, self.n_output)) - 1\n",
    "        }\n",
    "\n",
    "        self.biases = {\n",
    "            'h1': 2 * np.random.random((1, self.n_hidden)) - 1,\n",
    "            'out': 2 * np.random.random((1, self.n_output)) - 1\n",
    "        }\n",
    "    \n",
    "    # This function trains the network\n",
    "    def fit(self, X_train, Y_train):\n",
    "        Y_train[Y_train == -1] = 0\n",
    "        Y_train = np.expand_dims(Y_train, axis=1)\n",
    "        self.initialise_weights_biases(X_train, Y_train)\n",
    "        for iter in range(self.epochs):\n",
    "            # Forward Propagation\n",
    "            inputHidden = np.dot(X_train, self.weights['h1']) + self.biases['h1']\n",
    "            outputHidden = self.sig(inputHidden)\n",
    "            inputForOutputLayer = np.dot(outputHidden, self.weights['out']) + self.biases['out']\n",
    "            output = self.sig(inputForOutputLayer)\n",
    "\n",
    "            # Backpropagation\n",
    "            first_term_output_layer = output - Y_train\n",
    "            second_term_output_layer = self.derivativeSig(inputForOutputLayer)\n",
    "            first_two_output_layer = first_term_output_layer * second_term_output_layer\n",
    "            \n",
    "            first_term_hidden_layer = np.dot(first_two_output_layer, self.weights['out'].T)\n",
    "            second_term_hidden_layer = self.derivativeSig(inputHidden)\n",
    "            first_two_hidden_layer = first_term_hidden_layer * second_term_hidden_layer\n",
    "\n",
    "            changes_output = np.dot(outputHidden.T, first_two_output_layer)\n",
    "            changes_output_bias = np.sum(first_two_output_layer, axis = 0, keepdims = True)\n",
    "\n",
    "            changes_hidden = np.dot(X_train.T, first_two_hidden_layer)\n",
    "            changes_hidden_bias = np.sum(first_two_hidden_layer, axis = 0, keepdims = True)\n",
    "\n",
    "            self.weights['out'] = self.weights['out'] - self.learning_rate*changes_output\n",
    "            self.biases['out'] = self.biases['out'] - self.learning_rate*changes_output_bias\n",
    "\n",
    "            self.weights['h1'] = self.weights['h1'] - self.learning_rate*changes_hidden\n",
    "            self.biases['h1'] = self.biases['h1'] - self.learning_rate*changes_hidden_bias\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        inputHidden = np.dot(X_test, self.weights['h1']) + self.biases['h1']\n",
    "        outputHidden = self.sig(inputHidden)\n",
    "        inputForOutputLayer = np.dot(outputHidden, self.weights['out']) + self.biases['out']\n",
    "        predictions = self.sig(inputForOutputLayer)\n",
    "#         print(np.squeeze(predictions))\n",
    "        predictions = np.rint(np.squeeze(predictions))\n",
    "#         print(predictions)\n",
    "        predictions[predictions == 0] = -1\n",
    "#         print(predictions)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ready(symbol_name, max_features=5):\n",
    "    X_train, X_test, Y_train, Y_test = hm.prepare_data(max_features, symbol_name='AAPL')\n",
    "    X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "    return X_train, X_test, Y_train, Y_test    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SKLearn NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_MLP_forecast(X_train, X_test, Y_train, Y_test, n_hidden = 2, learning_rate = 0.01, epochs = 200):\n",
    "    print('SKLEARN INBUILT')\n",
    "    clf = MLPClassifier(max_iter=epochs, hidden_layer_sizes=(n_hidden))\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print('Predictions: ', clf.predict(X_test))\n",
    "    print('Accuracy Score --', clf.score(X_test, Y_test), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementation Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implemented_NN_forecast(X_train, X_test, Y_train, Y_test, n_hidden = 2, learning_rate = 0.01, epochs = 200):\n",
    "    nn = Neural_Network(n_hidden=n_hidden, learning_rate=learning_rate, epochs=epochs)\n",
    "    nn.fit(X_train, Y_train)\n",
    "    \n",
    "    print('IMPLEMENTATION') \n",
    "    Y_pred = nn.predict(X_test)\n",
    "    print('Y_test:', Y_test)\n",
    "    print('Y_pred:', Y_pred)\n",
    "    hm.accuracy_metrics(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(X_train, X_test, Y_train, Y_test, n_hidden = 2, learning_rate = 0.01, epochs = 200):\n",
    "    print('Number of Hidden Neurons --', n_hidden)\n",
    "    print('Learning Rate --', learning_rate)\n",
    "    print('Number of Epochs --', epochs, '\\n\\n')\n",
    "    sklearn_MLP_forecast(X_train, X_test, Y_train, Y_test, n_hidden, learning_rate, epochs)\n",
    "    implemented_NN_forecast(X_train, X_test, Y_train, Y_test, n_hidden, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_NN(symbol_name):\n",
    "    max_features = 5\n",
    "    n_hidden = 5\n",
    "    learning_rate = 0.1\n",
    "    epochs = 20\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = get_data_ready(symbol_name, max_features)\n",
    "#     X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "#     X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "#     Y_train = np.array([0,1,1,0])\n",
    "#     Y_test = np.array([0,1,1,0])\n",
    "    forecast(X_train, X_test, Y_train, Y_test, n_hidden, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hidden Neurons -- 5\n",
      "Learning Rate -- 0.1\n",
      "Number of Epochs -- 20 \n",
      "\n",
      "\n",
      "SKLEARN INBUILT\n",
      "Predictions:  [ 1.  1.  1. ...  1.  1. -1.]\n",
      "Accuracy Score -- 0.5152905198776758 \n",
      "\n",
      "\n",
      "IMPLEMENTATION\n",
      "Y_test: [-1. -1.  1. ... -1.  1. -1.]\n",
      "Y_pred: [1. 1. 1. ... 1. 1. 1.]\n",
      "Accuracy: 0.5061162079510704\n",
      "Matthews Correlation Coefficient: 0.0\n",
      "Confustion Matrix\n",
      "[[  0 646]\n",
      " [  0 662]]\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.00      0.00      0.00       646\n",
      "        1.0       0.51      1.00      0.67       662\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1308\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "run_NN(symbol_name = 'AAPL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
