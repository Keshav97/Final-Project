{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import helper_methods as hm\n",
    "import preprocessing as pp\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier Implementation\n",
    "http://tullo.ch/articles/svm-py/  \n",
    "https://pythonprogramming.net/soft-margin-kernel-cvxopt-svm-machine-learning-tutorial/  \n",
    "https://xavierbourretsicotte.github.io/SVM_implementation.html  \n",
    "http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/kernels.pdf  \n",
    "https://courses.csail.mit.edu/6.867/wiki/images/a/a7/Qp-cvxopt.pdf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function imitates a Polynomial Kernel with the dimension d as input to the function\n",
    "def polynomial_kernel(x1, x2, degree = 3):\n",
    "    return (1 + np.dot(x1, x2)) ** degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x1, x2, gamma):\n",
    "    if gamma == 'auto':\n",
    "        gamma = 1 / len(x1)\n",
    "    return np.exp(gamma * (-np.linalg.norm(x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Support_Vector_Machine_Classifier:\n",
    "    def __init__(self, C = 1.0, kernel = 'rbf', degree = 3, gamma = 'auto'):\n",
    "        # https://stackoverflow.com/questions/60208/replacements-for-switch-statement-in-python\n",
    "        self._kernel = {\n",
    "                        'linear': linear_kernel,\n",
    "                        'poly': lambda x1, x2: polynomial_kernel(x1, x2, degree = degree),\n",
    "                        'rbf': lambda x1, x2: rbf_kernel(x1, x2, gamma = gamma),\n",
    "                    }.get(kernel, rbf_kernel) \n",
    "        # to force the float type\n",
    "        self._C = float(C)\n",
    "        self._degree = degree\n",
    "        self._gamma = gamma\n",
    "        print('C used:', C)\n",
    "        \n",
    "    # https://stackoverflow.com/questions/1547145/defining-private-module-functions-in-python\n",
    "    def _get_gram_matrix(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = self._kernel(X[i], X[j])\n",
    "        return K  \n",
    "        \n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        \n",
    "        K = self._get_gram_matrix(X_train)\n",
    "        \n",
    "        # We want to maximise wrt alpha, instead we minimise the negative of the expression wrt alpha\n",
    "        # We want to maximise Q(a)\n",
    "        # Q(a) = -(1/2)a(T)Pa + sum_i(a_i)\n",
    "        # Instead we minimise (-Q(a))\n",
    "        # Check SVM slide 14 and 20\n",
    "        \n",
    "        # P = sum i,j(yi * yj * K(xi, xj))\n",
    "        P = cvxopt.matrix(np.outer(Y_train, Y_train) * K)\n",
    "        # q = -1\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        \n",
    "        # Ax = b ====> sum_i(y_i * a_i) = 0\n",
    "        A = cvxopt.matrix(Y_train, (1, n_samples))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        \n",
    "        # Gx <= h =====> 0 <= a <= C\n",
    "        # Top half -----> -a <= 0\n",
    "        top_half = np.eye(n_samples) * -1\n",
    "        # Bottom half -----> a <= C\n",
    "        bot_half = np.eye(n_samples)\n",
    "        G = cvxopt.matrix(np.vstack((top_half, bot_half)))\n",
    "        \n",
    "        top_half = np.zeros(n_samples)\n",
    "        bot_half = np.ones(n_samples) * self._C\n",
    "        h = cvxopt.matrix(np.hstack((top_half, bot_half)))\n",
    "        \n",
    "        # Silent cvxopt solver - https://stackoverflow.com/a/27184422/6001624\n",
    "        cvxopt.solvers.options['show_progress'] = True\n",
    "        \n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers (the alphas)\n",
    "        alphas = np.ravel(solution['x'])\n",
    "\n",
    "        # Support vectors have non zero lagrange multipliers (Check slide 22 SVM)\n",
    "        is_sv = alphas > 1e-5\n",
    "        sv_indices = np.arange(len(alphas))[is_sv]\n",
    "        self._alphas = alphas[is_sv]\n",
    "        \n",
    "        # the x part of the SVs\n",
    "        self._support_vectors = X_train[is_sv]\n",
    "        # the y part of the SVs\n",
    "        self._support_vector_labels = Y_train[is_sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self._alphas), n_samples))\n",
    "\n",
    "        # Slide 12: http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/kernels.pdf\n",
    "        # Intercept -----> b = mean(y_k - sum_i(a_i * y_i * x_i).x_k) for all k st 0 < a_k < C\n",
    "        #                    = mean(y_k - sum(a_i * y_i * K(x_i, x_k))) for all k st 0 < a_k < C\n",
    "        self._b = 0\n",
    "        for i in range(len(self._alphas)):\n",
    "            self._b += self._support_vector_labels[i]\n",
    "            self._b -= np.sum(self._alphas * self._support_vector_labels * K[sv_indices[i],is_sv]) # use is_sv as only those k st 0 < a_k < C\n",
    "        self._b /= len(self._alphas)\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y_predict = np.ones(len(X)) * self._b\n",
    "        for i in range(len(X)):\n",
    "            for alpha, sv_y, sv in zip(self._alphas, self._support_vector_labels, self._support_vectors):\n",
    "                y_predict[i] += alpha * sv_y * self._kernel(X[i], sv)\n",
    "        return np.sign(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Optimal Parameters\n",
    "https://stats.stackexchange.com/questions/43943/which-search-range-for-determining-svm-optimal-c-and-gamma-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameters_accuracy(accuracies):\n",
    "    print('#Features \\t C \\t\\t Gamma \\t\\t Accuracy')\n",
    "    for i in range(len(accuracies)):\n",
    "        print(accuracies[i][0], '\\t\\t', accuracies[i][1], '\\t\\t', accuracies[i][2], '\\t\\t', accuracies[i][3])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_values(max_features, C_list, gamma_list, kernel = 'rbf', num_splits = 10, symbol_name = 'AAPL', use_implementation = True):\n",
    "    accuracies = list()\n",
    "    for num_features in range(1, max_features + 1):\n",
    "        print('Features:', num_features)\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = hm.prepare_data(num_features, symbol_name, is_binary_ouput=True)\n",
    "        X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "        \n",
    "        for C in C_list: # [1, 10, 100, 500, 1000, 5000, 10000]:\n",
    "            for gamma in gamma_list: # [1, 0.1, 0.05, 0.01, 0.005, 0.001]:\n",
    "                print('(C, gamma) ------------------------> (' + str(C) + ', ' + str(gamma) + ')')\n",
    "                skl_svm_tscv = svm.SVC(kernel=kernel, C=C, gamma=gamma)\n",
    "#                 svm_tscv = Support_Vector_Machine_Classifier(C = C, kernel = kernel, gamma = gamma)\n",
    "                if use_implementation:\n",
    "                    accuracy = hm.timeSeriesCV(X_train, Y_train, num_splits, skl_svm_tscv, is_classification=True)\n",
    "                else:\n",
    "                    accuracy = hm.rolling_cross_validation(X_train, Y_train, num_splits, svm_tscv, is_classification=True)\n",
    "                print('#SVs = ', skl_svm_tscv.n_support_)\n",
    "                accuracies.append([num_features, C, gamma, accuracy])\n",
    "    \n",
    "    print_parameters_accuracy(accuracies)\n",
    "    \n",
    "    # Sorting the accuracies\n",
    "    accuracies.sort(reverse=True, key=lambda x: x[len(accuracies) - 1])\n",
    "    print_parameters_accuracy(accuracies)\n",
    "    \n",
    "    return accuracies[0][0], accuracies[0][1], accuracies[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ready(symbol_name, max_features, C_list, gamma_list, kernel):\n",
    "    start_time = time.time()\n",
    "#     num_features, C, gamma = find_optimal_values(max_features, C_list, gamma_list, kernel, \n",
    "#                                                  num_splits=10, symbol_name = symbol_name)\n",
    "#     num_features, C, gamma = 3, 100000000, 0.01\n",
    "    num_features, C, gamma = 4, 512, 2\n",
    "    end_time = time.time()\n",
    "    print('Time taken for Cross Validation:', end_time - start_time)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = hm.prepare_data(num_features, symbol_name)\n",
    "    X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "    return X_train, X_test, Y_train, Y_test, C, gamma    \n",
    "    \n",
    "#     X_train, X_test, Y_train, Y_test = hm.prepare_data(2)\n",
    "#     X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "#     return X_train, X_test, Y_train, Y_test, 1.0, 0.25   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SkLearn SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C = 1.0, kernel = 'rbf'):\n",
    "    print('SKLEARN INBUILT SVM')\n",
    "    clf = svm.SVC(kernel = kernel, C = C, gamma=gamma)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print('In-Built SVM (Accuracy) score -- kernel =', kernel, '--', clf.score(X_test, Y_test), '\\n')\n",
    "    print('Number of SVs =', len(clf.support_vectors_))\n",
    "    hm.accuracy_metrics(Y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predicting using Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implemented_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C = 1.0, kernel = 'rbf'):\n",
    "    my_SVM = Support_Vector_Machine_Classifier(C = C, kernel = kernel, gamma = gamma)\n",
    "    print('Implemenation Fit Starting...')\n",
    "    my_SVM.fit(X_train, Y_train)\n",
    "    \n",
    "    print('IMPLEMENTATION') \n",
    "    Y_pred = my_SVM.predict(X_test)\n",
    "    hm.accuracy_metrics(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(X_train, X_test, Y_train, Y_test, gamma, C = 1.0, kernel = 'rbf'):\n",
    "    print('C:', C, '\\t gamma:', gamma)\n",
    "    sklearn_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C, kernel = kernel)\n",
    "    implemented_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C, kernel)\n",
    "    \n",
    "# X_train, X_test, Y_train, Y_test = hm.prepare_data(2)\n",
    "# X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "# forecast(X_train, X_test, Y_train, Y_test, 0.25, 1, 'rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://m.svms.org/regression/TaCa01.pdf  \n",
    "Sensitivity of SVMs to parameters  \n",
    "Application of support vector machines in Financial time series forecasting  \n",
    "Francis E.H. Tay ∗, Lijuan Cao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SVM(symbol_name):\n",
    "    max_features = 10\n",
    "    kernel = 'rbf'\n",
    "    C_list = list()\n",
    "    for i in range(-5, 16):\n",
    "        C_list.append(2 ** i)\n",
    "    print(C_list)\n",
    "    gamma_list = list()\n",
    "    for i in range(-15, 4):\n",
    "        gamma_list.append(2 ** i)\n",
    "    \n",
    "#     C_list = [1, 10, 33, 55, 78, 100]\n",
    "#     gamma_list = [1/25, 1/10, 1, 10, 33, 55, 78, 100, 200, 300, 500, 1000, 2000, 3000]\n",
    "#     gamma_list = [1/100, 1/75, 1/50, 1/25, 1/10, 1]\n",
    "#     gamma_list = [1, 10, 25, 50, 75, 100]\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, C, gamma = get_data_ready(symbol_name, max_features, C_list, gamma_list, kernel)\n",
    "    forecast(X_train, X_test, Y_train, Y_test, gamma, C, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_SVM(symbol_name = 'AAPL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
