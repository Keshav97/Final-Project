{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import stats_helper as sh\n",
    "import preprocessing as pp\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kohen Cappa - Kappa or Cohenâ€™s Kappa is like classification accuracy, except that it is normalized at the baseline of random chance on your dataset. It is a more useful measure to use on problems that have an imbalance in the classes (e.g. 70-30 split for classes 0 and 1 and you can achieve 70% accuracy by predicting all instances are for class 0)  \n",
    "https://machinelearningmastery.com/machine-learning-evaluation-metrics-in-r/  \n",
    "\n",
    "The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Exponential Smoothing - Holt's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holt:\n",
    "    \"\"\"\n",
    "        data - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "        beta - float [0.0, 1.0], smoothing parameter for trend\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.result = []\n",
    "    \n",
    "    def fit(self, data_train):\n",
    "        self.data_train = data_train\n",
    "        \n",
    "        # First value is same as series\n",
    "        self.result.append(data_train[0])\n",
    "        for n in range(1, len(data_train) + 1):\n",
    "            \n",
    "            # Initialising level and trend\n",
    "            if n == 1:\n",
    "                level, trend = data_train[0], data_train[1] - data_train[0]\n",
    "            \n",
    "            # Forecasting the point ahead\n",
    "            if n >= len(data_train): \n",
    "                value = self.result[-1]\n",
    "            else:\n",
    "                value = data_train[n]\n",
    "            \n",
    "            last_level, level = level, self.alpha*value + (1 - self.alpha)*(level + trend)\n",
    "            trend = self.beta*(level - last_level) + (1 - self.beta)*trend\n",
    "            self.result.append(level + trend)\n",
    "\n",
    "        # Returning the smoothed values (without the forecast)\n",
    "        return self.result[:-1]\n",
    "    \n",
    "    # Returns the forecasted point during the fit\n",
    "    def predict_one(self):\n",
    "        return self.result[-1]\n",
    "\n",
    "    def predict(self, data_test, is_classification):\n",
    "        predictions = []\n",
    "        self.result.append(self.data_train[0])\n",
    "        for n in range(1, len(self.data_train) + len(data_test)):\n",
    "            if n == 1:\n",
    "                level, trend = self.data_train[0], self.data_train[1] - self.data_train[0]\n",
    "            if n >= len(self.data_train): # we are forecasting\n",
    "                value = data_test[n - len(self.data_train)]\n",
    "                \n",
    "                predict_value = self.result[-1]\n",
    "                \n",
    "                # Adding it for generating binary returns\n",
    "                if n == len(self.data_train) and is_classification:\n",
    "                    predictions.append(predict_value)\n",
    "                \n",
    "                predict_last_level, predict_level = level, self.alpha*predict_value + (1 - self.alpha)*(level + trend)\n",
    "                predict_trend = self.beta*(predict_level - predict_last_level) + (1 - self.beta)*trend\n",
    "                predictions.append(predict_level + predict_trend)\n",
    "            else:\n",
    "                value = self.data_train[n]\n",
    "                \n",
    "            last_level, level = level, self.alpha*value + (1 - self.alpha)*(level + trend)\n",
    "            trend = self.beta*(level - last_level) + (1 - self.beta)*trend\n",
    "            self.result.append(level + trend)\n",
    "        \n",
    "#         print(predictions)\n",
    "        if is_classification:\n",
    "            return sh.output_to_binary_indicators(sh.data_daily_returns(predictions))\n",
    "        \n",
    "        return predictions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Holt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDoubleExponentialSmoothing(data, alphas, betas):\n",
    "    \"\"\"\n",
    "        Plots double exponential smoothing with different alphas and betas\n",
    "        \n",
    "        data - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters for level\n",
    "        betas - list of floats, smoothing parameters for trend\n",
    "    \"\"\"\n",
    "    \n",
    "    with plt.style.context('seaborn-white'):    \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        for alpha in alphas:\n",
    "            for beta in betas:\n",
    "                holt = Holt(alpha, beta)\n",
    "                plt.plot(holt.fit(data), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "        plt.plot(data, label = \"Actual\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.title(\"Double Exponential Smoothing - Holt\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "# plotDoubleExponentialSmoothing(values, alphas=[0.8, 0.05], betas=[0.8, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHolt(data, fit_data):\n",
    "    plt.figure(figsize=(20, 7))\n",
    "    plt.plot(range(len(data)), data, 'bo-')\n",
    "    plt.plot(range(len(fit_data)), fit_data, 'r^--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric_values(metric_values):\n",
    "    print('Alpha \\t\\t\\t Beta \\t\\t\\t Metric')\n",
    "    for i in range(5): #range(len(metric_values)):\n",
    "        print(metric_values[i][0], '\\t\\t\\t', metric_values[i][1], '\\t\\t\\t', metric_values[i][2])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_parameters(data_train, num_splits, alphas, betas, is_classification):\n",
    "    metric_values = list()\n",
    "    if is_classification:\n",
    "        print('Using Accuracy for CV')\n",
    "    else:\n",
    "        print('Using Mean Squared Error for CV')\n",
    "        \n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "#             print(\"Alpha {}, Beta {}\".format(alpha, beta))\n",
    "            holt = Holt(alpha, beta)\n",
    "            metric_value = sh.statTimeSeriesCV(data_train, num_splits, holt, is_classification)\n",
    "            metric_values.append([alpha, beta, metric_value])\n",
    "    \n",
    "#     print_metric_values(metric_values)\n",
    "    \n",
    "    # Sorting the Metric Values\n",
    "    metric_values.sort(reverse=is_classification, key=lambda x: x[len(metric_values) - 1])\n",
    "    print_metric_values(metric_values)\n",
    "    \n",
    "    return metric_values[0][0], metric_values[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alpha:  0.99 \t\t\t Beta:  0.0 \t\t\t RMS:  1.778531300846033\n",
    "# train_data, test_data = series_split(values)\n",
    "# min_rms = 1000\n",
    "# min_alpha = 0\n",
    "# min_beta = 0\n",
    "# for alpha in np.arange(0.0, 1.0, 0.01):\n",
    "#     for beta in np.arange(0.0, 1.0, 0.01):\n",
    "#         holt = Holt(alpha, beta)\n",
    "#         updated = holt.predict_and_update(series_train=train_data, series_test=test_data)\n",
    "#         rms = math.sqrt(mean_squared_error(test_data, updated))\n",
    "#         if rms < min_rms:\n",
    "#             min_rms = rms\n",
    "#             min_alpha = alpha\n",
    "#             min_beta = beta\n",
    "# print('Alpha: ', min_alpha, '\\t\\t\\t Beta: ', min_beta, '\\t\\t\\t RMS: ', min_rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns and Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Alpha:  0.14 \t\t Beta:  0.86 \t\t acc:  0.5362872421695951\n",
    "# train_data, test_data = series_split(values)\n",
    "# bin_test_data = output_to_binary_indicators(data_daily_returns(train_data[-1:] + test_data))\n",
    "# max_acc = 0\n",
    "# max_alpha = 0\n",
    "# max_beta = 0\n",
    "# for alpha in np.arange(0.01, 1.0, 0.01):\n",
    "#     for beta in np.arange(0.0, 1.0, 0.01):\n",
    "#         holt = Holt(alpha, beta)\n",
    "#         predictions = holt.predict_and_update(train_data, test_data[:])\n",
    "#         acc = accuracy_score(bin_test_data, predictions)\n",
    "#         if acc > max_acc:\n",
    "#             max_acc = acc\n",
    "#             max_alpha = alpha\n",
    "#             max_beta = beta\n",
    "# print('Alpha: ', max_alpha, '\\t\\t Beta: ', max_beta, '\\t\\t acc: ', max_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Metric Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Holt for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(data_train, data_test, alpha, beta, is_classification):\n",
    "    print('Alpha: ', alpha, '\\t Beta: ', beta)\n",
    "    \n",
    "    holt = Holt(alpha, beta)\n",
    "    print('Fitting...')\n",
    "    holt.fit(data_train)\n",
    "    \n",
    "    print('Predicting...') \n",
    "    predictions = holt.predict(data_test, is_classification)\n",
    "    \n",
    "#     print(predictions)\n",
    "    \n",
    "    if is_classification:\n",
    "        bin_data_test = sh.output_to_binary_indicators(sh.data_daily_returns(data_train[-1:] + data_test))\n",
    "        sh.classification_metrics(bin_data_test, predictions)\n",
    "    else:\n",
    "        sh.regression_metrics(data_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ready(symbol_name, alphas, betas, is_classification):\n",
    "    start_time = time.time()\n",
    "    num_splits = 10\n",
    "    print('Data Prep')\n",
    "    data_train, data_test = sh.prepare_data(symbol_name, train_ratio = 0.8)\n",
    "#     alpha, beta = find_optimal_parameters(data_train, num_splits, alphas, betas, is_classification)\n",
    "    alpha, beta = 0.8, 0.6\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('Time taken for Cross Validation:', end_time - start_time)\n",
    "    return data_train, data_test, alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Holt(symbol_name):\n",
    "    is_classification = False\n",
    "    alphas = np.arange(0, 1, 0.01)\n",
    "    betas = np.arange(0, 1, 0.01)\n",
    "    \n",
    "    data_train, data_test, alpha, beta = get_data_ready(symbol_name, alphas, betas, is_classification)\n",
    "    forecast(data_train, data_test, alpha, beta, is_classification = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_Holt(symbol_name = 'AMZN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
