{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import stats_helper as sh\n",
    "import preprocessing as pp\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kohen Cappa - Kappa or Cohenâ€™s Kappa is like classification accuracy, except that it is normalized at the baseline of random chance on your dataset. It is a more useful measure to use on problems that have an imbalance in the classes (e.g. 70-30 split for classes 0 and 1 and you can achieve 70% accuracy by predicting all instances are for class 0)  \n",
    "https://machinelearningmastery.com/machine-learning-evaluation-metrics-in-r/  \n",
    "\n",
    "The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Damp:\n",
    "    \"\"\"\n",
    "        data - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "        beta - float [0.0, 1.0], smoothing parameter for trend\n",
    "        theta - float [0.0, 1.0], damping paramater\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, beta, theta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.theta = theta\n",
    "        self.result = []\n",
    "    \n",
    "    def fit(self, data_train):\n",
    "        self.data_train = data_train\n",
    "        \n",
    "        # First value is same as series\n",
    "        self.result.append(data_train[0])\n",
    "        for n in range(1, len(data_train) + 1):\n",
    "            \n",
    "            # Initialising level and trend\n",
    "            if n == 1:\n",
    "                level, trend = data_train[0], data_train[1] - data_train[0]\n",
    "            \n",
    "            # Forecasting the point ahead\n",
    "            if n >= len(data_train): \n",
    "                value = self.result[-1]\n",
    "            else:\n",
    "                value = data_train[n]\n",
    "            \n",
    "            last_level, level = level, self.alpha*value + (1 - self.alpha)*(level + self.theta*trend)\n",
    "            trend = self.beta*(level - last_level) + (1 - self.beta)*self.theta*trend\n",
    "            self.result.append(level + self.theta*trend)\n",
    "\n",
    "        # Returning the smoothed values (without the forecast)\n",
    "        return self.result[:-1]\n",
    "    \n",
    "    # Returns the forecasted point during the fit\n",
    "    def predict_one(self):\n",
    "        return self.result[-1]\n",
    "\n",
    "    def predict(self, data_test, is_classification):\n",
    "        predictions = []\n",
    "        self.result.append(self.data_train[0])\n",
    "        for n in range(1, len(self.data_train) + len(data_test)):\n",
    "            if n == 1:\n",
    "                level, trend = self.data_train[0], self.data_train[1] - self.data_train[0]\n",
    "            if n >= len(self.data_train): # we are forecasting\n",
    "                value = data_test[n - len(self.data_train)]\n",
    "                \n",
    "                predict_value = self.result[-1]\n",
    "                \n",
    "                # Adding it for generating binary returns\n",
    "                if n == len(self.data_train) and is_classification:\n",
    "                    predictions.append(predict_value)\n",
    "                \n",
    "                predict_last_level, predict_level = level, self.alpha*predict_value + (1 - self.alpha)*(level + self.theta*trend)\n",
    "                predict_trend = self.beta*(predict_level - predict_last_level) + (1 - self.beta)*self.theta*trend\n",
    "                predictions.append(predict_level + self.theta*predict_trend)\n",
    "            else:\n",
    "                value = self.data_train[n]\n",
    "                \n",
    "            last_level, level = level, self.alpha*value + (1 - self.alpha)*(level + self.theta*trend)\n",
    "            trend = self.beta*(level - last_level) + (1 - self.beta)*self.theta*trend\n",
    "            self.result.append(level + self.theta*trend)\n",
    "        \n",
    "#         print(predictions)\n",
    "        if is_classification:\n",
    "            return sh.output_to_binary_indicators(sh.data_daily_returns(predictions))\n",
    "        \n",
    "        return predictions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric_values(metric_values):\n",
    "    print('Alpha \\t\\t\\t Beta \\t\\t\\t Theta \\t\\t\\t Metric')\n",
    "    for i in range(5): #range(len(metric_values)):\n",
    "        print(metric_values[i][0], '\\t\\t\\t', metric_values[i][1], '\\t\\t\\t', metric_values[i][2], '\\t\\t\\t', metric_values[i][3])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_parameters(data_train, num_splits, alphas, betas, thetas, is_classification):\n",
    "    metric_values = list()\n",
    "    if is_classification:\n",
    "        print('Using Accuracy for CV')\n",
    "    else:\n",
    "        print('Using Mean Squared Error for CV')\n",
    "        \n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            for theta in thetas:\n",
    "#                 print(\"Alpha {}, Beta {}, Theta {}\".format(alpha, beta, theta))\n",
    "                damp = Damp(alpha, beta, theta)\n",
    "                metric_value = sh.statTimeSeriesCV(data_train, num_splits, damp, is_classification)\n",
    "                metric_values.append([alpha, beta, theta, metric_value])\n",
    "    \n",
    "#     print_metric_values(metric_values)\n",
    "    \n",
    "    # Sorting the Metric Values\n",
    "    metric_values.sort(reverse=is_classification, key=lambda x: x[len(metric_values) - 1])\n",
    "    print_metric_values(metric_values)\n",
    "    \n",
    "    return metric_values[0][0], metric_values[0][1], metric_values[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Damp for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(data_train, data_test, alpha, beta, theta, is_classification):\n",
    "#     print('Alpha: ', alpha, '\\t Beta: ', beta, '\\t Theta: ', theta)\n",
    "    \n",
    "    damp = Damp(alpha, beta, theta)\n",
    "    print('Fitting...')\n",
    "    damp.fit(data_train)\n",
    "    \n",
    "    print('Predicting...') \n",
    "    predictions = damp.predict(data_test, is_classification)\n",
    "    \n",
    "#     print(predictions)\n",
    "    \n",
    "    if is_classification:\n",
    "        bin_data_test = sh.output_to_binary_indicators(sh.data_daily_returns(data_train[-1:] + data_test))\n",
    "        sh.classification_metrics(bin_data_test, predictions)\n",
    "    else:\n",
    "        sh.regression_metrics(data_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ready(symbol_name, alphas, betas, thetas, is_classification):\n",
    "    start_time = time.time()\n",
    "    num_splits = 10\n",
    "    print('Data Prep')\n",
    "    data_train, data_test = sh.prepare_data(symbol_name, train_ratio = 0.8)\n",
    "    alpha, beta, theta = find_optimal_parameters(data_train, num_splits, alphas, betas, thetas, is_classification)\n",
    "#     alpha, beta, theta = 0.8, 0.6, 0.5\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('Time taken for Cross Validation:', end_time - start_time)\n",
    "    return data_train, data_test, alpha, beta, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Damp(symbol_name):\n",
    "    is_classification = False\n",
    "    alphas = np.arange(0, 1, 0.1)\n",
    "    betas = np.arange(0, 1, 0.1)\n",
    "    thetas = np.arange(0, 1, 0.1)\n",
    "    \n",
    "    data_train, data_test, alpha, beta, theta = get_data_ready(symbol_name, alphas, betas, thetas, is_classification)\n",
    "    forecast(data_train, data_test, alpha, beta, theta, is_classification = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_Damp(symbol_name = 'AMZN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
