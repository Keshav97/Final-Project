{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from helper_methods.ipynb\n",
      "Importing Jupyter notebook from preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "import helper_methods as hm\n",
    "import preprocessing as pp\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxopt\n",
    "from numpy import linalg\n",
    "import cvxopt.solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier Implementation\n",
    "16384\n",
    ".125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def polynomial_kernel(x, y, p=3):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def gaussian_kernel(x, y, gamma = 0.25):\n",
    "    return np.exp(-linalg.norm(x-y)**2 * gamma)\n",
    "#     return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=linear_kernel, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = self.kernel(X[i], X[j])\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == linear_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def project(self, X):\n",
    "        if self.w is not None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * self.kernel(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Optimal Parameters\n",
    "https://stats.stackexchange.com/questions/43943/which-search-range-for-determining-svm-optimal-c-and-gamma-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameters_accuracy(accuracies):\n",
    "    print('#Features \\t C \\t Gamma \\t Accuracy')\n",
    "    for i in range(len(accuracies)):\n",
    "        print(accuracies[i][0], '\\t\\t', accuracies[i][1], '\\t\\t', accuracies[i][2], '\\t\\t', accuracies[i][3])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_values(max_features, kernel = 'rbf', num_splits = 10, symbol_name = 'AAPL', use_implementation = True):\n",
    "    accuracies = list()\n",
    "    for num_features in range(1, max_features + 1, 1):\n",
    "        print('Features:', num_features)\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = hm.prepare_data(num_features, symbol_name, is_binary_ouput=True)\n",
    "        X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "        \n",
    "        gamma = 0.25\n",
    "        \n",
    "#         for C in [1, 10, 100, 500, 1000, 5000, 10000]:\n",
    "        for C in [2 ** x for x in range(-4, 8)]:\n",
    "#             for gamma in [1, 0.1, 0.05, 0.01, 0.005, 0.001]:\n",
    "#         for gamma in [2 ** x for x in range(-15, 3)]:\n",
    "            print('(C, gamma) ------------------------> (' + str(C) + ', ' + str(gamma) + ')')\n",
    "#             skl_svm_tscv = svm.SVC(kernel=kernel, C=C, gamma=gamma)\n",
    "            skl_svm_tscv = SVM(gaussian_kernel, C = C)\n",
    "            if use_implementation:\n",
    "                accuracy = hm.timeSeriesCV(X_train, Y_train, num_splits, skl_svm_tscv, is_classification=True)\n",
    "            else:\n",
    "                accuracy = hm.rolling_cross_validation(X_train, Y_train, num_splits, skl_svm_tscv, is_classification=True)\n",
    "            accuracies.append([num_features, C, gamma, accuracy])\n",
    "    \n",
    "    print_parameters_accuracy(accuracies)\n",
    "    \n",
    "    # Sorting the accuracies\n",
    "    accuracies.sort(reverse=True, key=lambda x: x[len(accuracies) - 1])\n",
    "    print_parameters_accuracy(accuracies)\n",
    "    \n",
    "    return accuracies[0][0], accuracies[0][1], accuracies[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ready(symbol_name):\n",
    "#     num_features, C, gamma = find_optimal_values(max_features=2, kernel = 'rbf', num_splits=10, symbol_name = symbol_name)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = hm.prepare_data(2)\n",
    "    X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "    return X_train, X_test, Y_train, Y_test, 1.0, 0.25    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SkLearn SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C = 1.0, kernel = 'rbf'):\n",
    "    print('SKLEARN INBUILT SVM')\n",
    "    clf = svm.SVC(kernel = kernel, C = C, gamma=gamma)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print('In-Built SVM (Accuracy) score -- kernel =', kernel, '--', clf.score(X_test, Y_test), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predicting using Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implemented_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C = 1.0, kernel = 'rbf'):\n",
    "    my_SVM = SVM(linear_kernel, C = C)\n",
    "    my_SVM.fit(X_train, Y_train)\n",
    "    \n",
    "    print('IMPLEMENTATION') \n",
    "    Y_pred = my_SVM.predict(X_test)\n",
    "    print('Accuracy Score --', accuracy_score(Y_test, Y_pred))\n",
    "    print('Matthews Correlation Coefficient --', matthews_corrcoef(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(X_train, X_test, Y_train, Y_test, gamma, C = 1.0):\n",
    "    print('C:', C, '\\t gamma:', gamma)\n",
    "    sklearn_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C, kernel = 'rbf')\n",
    "    implemented_SVM_forecast(X_train, X_test, Y_train, Y_test, gamma, C, kernel = 'rbf')\n",
    "    \n",
    "# X_train, X_test, Y_train, Y_test = hm.prepare_data(2)\n",
    "# X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "# forecast(X_train, X_test, Y_train, Y_test, 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SVM(symbol_name):\n",
    "    X_train, X_test, Y_train, Y_test, C, gamma = get_data_ready(symbol_name)\n",
    "    forecast(X_train, X_test, Y_train, Y_test, gamma, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 1.0 \t gamma: 0.25\n",
      "SKLEARN INBUILT SVM\n",
      "In-Built SVM (Accuracy) score -- kernel = rbf -- 0.4938837920489297 \n",
      "\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.9228e+03 -7.9438e+03  2e+04  1e+00  3e-15\n",
      " 1: -3.2345e+03 -5.3787e+03  2e+03  1e-13  3e-15\n",
      " 2: -3.8491e+03 -3.9475e+03  1e+02  5e-14  3e-15\n",
      " 3: -3.9203e+03 -3.9240e+03  4e+00  4e-14  3e-15\n",
      " 4: -3.9210e+03 -3.9216e+03  6e-01  7e-14  3e-15\n",
      " 5: -3.9211e+03 -3.9216e+03  5e-01  7e-14  2e-15\n",
      " 6: -3.9211e+03 -3.9216e+03  5e-01  1e-13  2e-15\n",
      " 7: -3.9212e+03 -3.9215e+03  4e-01  7e-14  2e-15\n",
      " 8: -3.9212e+03 -3.9215e+03  4e-01  2e-13  2e-15\n",
      " 9: -3.9212e+03 -3.9215e+03  4e-01  1e-14  2e-15\n",
      "10: -3.9212e+03 -3.9215e+03  4e-01  1e-13  2e-15\n",
      "11: -3.9213e+03 -3.9214e+03  1e-01  1e-13  3e-15\n",
      "12: -3.9213e+03 -3.9214e+03  9e-02  2e-14  2e-15\n",
      "13: -3.9213e+03 -3.9214e+03  9e-02  7e-14  2e-15\n",
      "14: -3.9213e+03 -3.9213e+03  5e-03  5e-14  3e-15\n",
      "15: -3.9213e+03 -3.9213e+03  1e-04  2e-13  3e-15\n",
      "Optimal solution found.\n",
      "3924 support vectors out of 3924 points\n",
      "IMPLEMENTATION\n",
      "Accuracy Score -- 0.5030581039755352\n",
      "Matthews Correlation Coefficient -- 0.006417872908271325\n"
     ]
    }
   ],
   "source": [
    "run_SVM(symbol_name = 'INX')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
