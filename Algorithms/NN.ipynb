{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nbimporter\n",
    "import numpy as np\n",
    "from scikitplot.metrics import plot_roc\n",
    "import preprocessing as pp\n",
    "import helper_methods as hm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Classifier Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    \n",
    "    def __init__(self, n_hidden = 2, learning_rate = 0.01, epochs = 10000, batch_size = -1):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    # ACTIVATION - Sigmoid Function\n",
    "    def sig(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Derivative of Sigmoid Function\n",
    "    def derivativeSig(self, z):\n",
    "        return self.sig(z) * (1 - self.sig(z))\n",
    "    \n",
    "    def initialise_weights_biases(self, X_train, Y_train):\n",
    "        # Number of input neurons equal the number of input features\n",
    "        self.n_input = len(X_train[0])\n",
    "        \n",
    "        # Need only single output neuron to decide up or down\n",
    "        self.n_output = 1\n",
    "        \n",
    "        xavier_limits = [\n",
    "            (6 / (self.n_input + self.n_hidden)) ** 0.5,\n",
    "            (6 / (self.n_hidden + self.n_output)) ** 0.5\n",
    "        ]\n",
    "        \n",
    "        # Initialising weights and biases using Xavier Initialisation\n",
    "        # Reference: NLP Goldberg\n",
    "        self.weights = {\n",
    "#             'h1': np.random.random((self.n_input, self.n_hidden)),\n",
    "#             'out': np.random.random((self.n_hidden, self.n_output))\n",
    "            'h1': np.random.uniform(-xavier_limits[0], xavier_limits[0], (self.n_input, self.n_hidden)),\n",
    "            'out': np.random.uniform(-xavier_limits[1], xavier_limits[1], (self.n_hidden, self.n_output))\n",
    "        }\n",
    "\n",
    "        self.biases = {\n",
    "#             'h1': np.random.random((1, self.n_hidden)),\n",
    "#             'out': np.random.random((1, self.n_output))\n",
    "            'h1': np.random.uniform(-xavier_limits[0], xavier_limits[0], (1, self.n_hidden)),\n",
    "            'out': np.random.uniform(-xavier_limits[1], xavier_limits[1], (1, self.n_output))\n",
    "        }\n",
    "        \n",
    "    # getting the next batch for mini-batch gradient descent\n",
    "    def next_batch(self, X, y, batch_size):\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            yield (X[i: i + batch_size], y[i: i + batch_size])\n",
    "    \n",
    "    # This function trains the network\n",
    "    def fit(self, X_train, Y_train):\n",
    "#         self.batch_size = int(len(X)/10 if self.batch_size == -1 else self.batch_size)\n",
    "        \n",
    "        Y_train[Y_train == -1] = 0\n",
    "        Y_train = np.expand_dims(Y_train, axis=1)\n",
    "        \n",
    "        self.initialise_weights_biases(X_train, Y_train)\n",
    "        \n",
    "        for iter in range(self.epochs):\n",
    "            # Total loss for each epoch\n",
    "            epoch_loss = []\n",
    "            \n",
    "#             for (batch_X, batch_y) in self.next_batch(X_train, Y_train, self.batch_size): \n",
    "            # Forward Propagation\n",
    "            inputHidden = np.dot(X_train, self.weights['h1']) + self.biases['h1']\n",
    "            outputHidden = self.sig(inputHidden)\n",
    "            inputForOutputLayer = np.dot(outputHidden, self.weights['out']) + self.biases['out']\n",
    "            output = self.sig(inputForOutputLayer)\n",
    "\n",
    "            # Appending losses of the epoch\n",
    "            epoch_loss.append(np.sum((output - Y_train) ** 2))\n",
    "\n",
    "            # Backpropagation\n",
    "            # derivative of sum squared error divided by 2\n",
    "            first_term_output_layer = output - Y_train\n",
    "            second_term_output_layer = self.derivativeSig(inputForOutputLayer)\n",
    "            first_two_output_layer = first_term_output_layer * second_term_output_layer\n",
    "\n",
    "            first_term_hidden_layer = np.dot(first_two_output_layer, self.weights['out'].T)\n",
    "            second_term_hidden_layer = self.derivativeSig(inputHidden)\n",
    "            first_two_hidden_layer = first_term_hidden_layer * second_term_hidden_layer\n",
    "\n",
    "            changes_output = np.dot(outputHidden.T, first_two_output_layer)\n",
    "            changes_output_bias = np.sum(first_two_output_layer, axis = 0, keepdims = True)\n",
    "\n",
    "            changes_hidden = np.dot(X_train.T, first_two_hidden_layer)\n",
    "            changes_hidden_bias = np.sum(first_two_hidden_layer, axis = 0, keepdims = True)\n",
    "\n",
    "            self.weights['out'] = self.weights['out'] - self.learning_rate*changes_output\n",
    "            self.biases['out'] = self.biases['out'] - self.learning_rate*changes_output_bias\n",
    "\n",
    "            self.weights['h1'] = self.weights['h1'] - self.learning_rate*changes_hidden\n",
    "            self.biases['h1'] = self.biases['h1'] - self.learning_rate*changes_hidden_bias\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        inputHidden = np.dot(X_test, self.weights['h1']) + self.biases['h1']\n",
    "        outputHidden = self.sig(inputHidden)\n",
    "        inputForOutputLayer = np.dot(outputHidden, self.weights['out']) + self.biases['out']\n",
    "        predictions = self.sig(inputForOutputLayer)\n",
    "#         print('Not rounding', np.squeeze(predictions))\n",
    "        predictions = np.rint(np.squeeze(predictions))\n",
    "        predictions[predictions == 0] = -1\n",
    "        return predictions\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        inputHidden = np.dot(X_test, self.weights['h1']) + self.biases['h1']\n",
    "        outputHidden = self.sig(inputHidden)\n",
    "        inputForOutputLayer = np.dot(outputHidden, self.weights['out']) + self.biases['out']\n",
    "        predictions = self.sig(inputForOutputLayer)\n",
    "        prob = []\n",
    "        for prediction in predictions:\n",
    "            prob.append([1 - prediction[0], prediction[0]])\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data / Using CV to find Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameters_accuracy(accuracies):\n",
    "    print('#Features \\t #Hidden Neurons \\t Learning Rate \\t Accuracy')\n",
    "    for i in range(len(accuracies)):\n",
    "        print(accuracies[i][0], '\\t\\t', accuracies[i][1], '\\t\\t', accuracies[i][2], '\\t\\t', accuracies[i][3])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_values(max_features, n_hidden_list, lr_list, epochs, num_splits = 10, symbol_name = 'AAPL', use_implementation = True):\n",
    "    accuracies = list()\n",
    "    for num_features in range(1, max_features + 1):\n",
    "        print('Features:', num_features)\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = hm.prepare_data(num_features, symbol_name, is_binary_ouput=True)\n",
    "        X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "        \n",
    "        n_hidden_list = [(num_features + 1)//2 if (num_features % 2 != 0) else num_features//2 , num_features, 2*num_features, 2*num_features + 1]\n",
    "        print(n_hidden_list)\n",
    "        for n_hidden in n_hidden_list: # [5, 10, 20, 50, 100, 1000, 2000]: \n",
    "            for lr in lr_list: # [0.0001, 0.0005, 0.001, 0.01, 0.1, 1]:\n",
    "                print('(n_hidden, lr) ------------------------> (' + str(n_hidden) + ', ' + str(lr) + ')')\n",
    "#                 nn_tscv = Neural_Network(n_hidden=n_hidden, learning_rate=lr, epochs=epochs)\n",
    "                nn_tscv = MLPClassifier(max_iter=epochs, hidden_layer_sizes=(n_hidden))\n",
    "                if use_implementation:\n",
    "                    accuracy = hm.timeSeriesCV(X_train, Y_train, num_splits, nn_tscv, is_classification=True)\n",
    "                else:\n",
    "                    accuracy = hm.rolling_cross_validation(X_train, Y_train, num_splits, nn_tscv, is_classification=True)\n",
    "                accuracies.append([num_features, n_hidden, lr, accuracy])\n",
    "    \n",
    "    print_parameters_accuracy(accuracies)\n",
    "    \n",
    "    # Sorting the accuracies\n",
    "    accuracies.sort(reverse=True, key=lambda x: x[len(accuracies) - 1])\n",
    "    print_parameters_accuracy(accuracies)\n",
    "    \n",
    "    return accuracies[0][0], accuracies[0][1], accuracies[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ready(symbol_name, max_features, n_hidden_list, lr_list, epochs):\n",
    "    start_time = time.time()\n",
    "#     num_features, n_hidden, learning_rate = find_optimal_values(max_features, n_hidden_list, lr_list, epochs, num_splits=10, symbol_name = symbol_name)\n",
    "#     num_features, n_hidden, learning_rate = 2, 4, 0.01\n",
    "    num_features, n_hidden, learning_rate = 2, 5, 0.05\n",
    "    end_time = time.time()\n",
    "    print('Time taken for Cross Validation:', end_time - start_time)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = hm.prepare_data(max_features, symbol_name=symbol_name)\n",
    "    X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values\n",
    "    return X_train, X_test, Y_train, Y_test, n_hidden, learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SKLearn NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_MLP_forecast(X_train, X_test, Y_train, Y_test, n_hidden = 2, learning_rate = 0.01, epochs = 200):\n",
    "#     print('SKLEARN INBUILT')\n",
    "    clf = MLPClassifier(max_iter=epochs, hidden_layer_sizes=(n_hidden), learning_rate_init=learning_rate, solver='sgd')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print('Predictions: ', clf.predict(X_test))\n",
    "    hm.accuracy_metrics(Y_test, clf.predict(X_test))\n",
    "    print('Accuracy Score --', clf.score(X_test, Y_test), '\\n\\n')\n",
    "    y_pred_proba = clf.predict_proba(X_test)    \n",
    "    plot_roc(Y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementation Classifier\n",
    "\n",
    "ROC Curve: https://stackoverflow.com/a/42392458/6001624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implemented_NN_forecast(X_train, X_test, Y_train, Y_test, n_hidden = 2, learning_rate = 0.01, epochs = 200):\n",
    "    nn = Neural_Network(n_hidden=n_hidden, learning_rate=learning_rate, epochs=epochs, batch_size=len(X_train) / 10)\n",
    "    nn.fit(X_train, Y_train)\n",
    "    \n",
    "    print('IMPLEMENTATION') \n",
    "    Y_pred = nn.predict(X_test)\n",
    "    \n",
    "    hm.accuracy_metrics(Y_test, Y_pred)\n",
    "    \n",
    "    y_pred_proba = nn.predict_proba(X_test)\n",
    "    plot_roc(Y_test, y_pred_proba)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(X_train, X_test, Y_train, Y_test, n_hidden = 2, learning_rate = 0.01, epochs = 200):\n",
    "    print('Number of Hidden Neurons --', n_hidden)\n",
    "    print('Learning Rate --', learning_rate)\n",
    "    print('Number of Epochs --', epochs, '\\n\\n')\n",
    "    sklearn_MLP_forecast(X_train, X_test, Y_train, Y_test, n_hidden, learning_rate, epochs)\n",
    "#     implemented_NN_forecast(X_train, X_test, Y_train, Y_test, n_hidden, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_NN(symbol_name):\n",
    "    max_features = 20\n",
    "    epochs = 2000 # can plot the error against epochs to see the ideal number of epochs\n",
    "    # Reference: Number of hidden neurons : https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "    n_hidden_list = [5, 10, 20, 50, 100] #, 1000, 2000]\n",
    "    lr_list = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, n_hidden, learning_rate = get_data_ready(symbol_name, max_features, n_hidden_list, lr_list, epochs)\n",
    "    forecast(X_train, X_test, Y_train, Y_test, n_hidden, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_NN(symbol_name = 'AAPL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
