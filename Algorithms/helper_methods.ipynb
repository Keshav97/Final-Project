{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preprocessing as pp\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, accuracy_score, confusion_matrix, classification_report, matthews_corrcoef, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fumction calculates and returns the Mean Squared Error (MSE) for a model\n",
    "def mean_squared_error(Y_true, Y_pred):\n",
    "    # number of test samples\n",
    "    M = len(Y_true)\n",
    "    total_error = 0\n",
    "    for i in range(M):\n",
    "        total_error += (Y_true[i] - Y_pred[i]) ** 2\n",
    "    mse = total_error / M\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://clevertap.com/blog/the-best-metric-to-measure-accuracy-of-classification-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metrics(Y_test, Y_pred):\n",
    "    print('Accuracy: ', accuracy_score(Y_test, Y_pred))\n",
    "    print('Matthews Correlation Coefficient: ', matthews_corrcoef(Y_test, Y_pred))\n",
    "    print('Cohen Kappa Score: ', cohen_kappa_score(Y_test, Y_pred))\n",
    "    print('Confustion Matrix')\n",
    "    print(confusion_matrix(Y_test, Y_pred))\n",
    "    print('Classification Report')\n",
    "    print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits the data into train and test sets based on the split ratio supplied and \n",
    "# returns the corresponding X and Y values for the train and test data\n",
    "def train_test_split(df, train_ratio = 0.75):\n",
    "    split_index = int(train_ratio * len(df.index))\n",
    "    train_data = df.iloc[0:split_index, :]\n",
    "    test_data = df.iloc[split_index:, :]\n",
    "    \n",
    "    X_train, Y_train = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
    "    X_test, Y_test = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits the input data into corresponding X(features) and Y(output) values\n",
    "# The Y(output) values are assumed to be in the last column of the input data\n",
    "def get_X_Y(data):\n",
    "    X_data = list()\n",
    "    Y_data = list()\n",
    "    for i in data:\n",
    "        X_data.append(i[:-1])\n",
    "        Y_data.append(i[-1])\n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts the returns value to binary outputs for Classification\n",
    "# 1: Positive Return, -1: Negative Return\n",
    "def output_to_binary_indicators(df):\n",
    "    output_column_index = len(df.columns) - 1\n",
    "    for i in range(len(df)):\n",
    "        df.iloc[i, output_column_index] = 1 if df.iloc[i, output_column_index] >= 0 else -1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function adds the lag returns for the past num_lag days returns as features\n",
    "def add_lag_returns(df, data, num_lags):\n",
    "    # number of datapoints in the data frame\n",
    "    M = len(df.index)\n",
    "    \n",
    "    # creating columns for each of the lagged returns\n",
    "    col_names = [('Lag-' + str(i)) for i in range(num_lags, 0, -1)]\n",
    "    \n",
    "    for i in range(num_lags):\n",
    "        # gets the indices of the datapoints to build the features corresponding to RETURNS at t = t0 - (num_features - i)\n",
    "        indices_to_fetch = df.index[i:(M - num_lags + i)]\n",
    "        temp = df.loc[indices_to_fetch, 'RETURNS']\n",
    "        temp.index = df.index[num_lags:]\n",
    "        data[col_names[i]] = temp\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function adds the moving average returns for 2 to the past num_days returns as features\n",
    "def add_moving_average_returns(df, data, num_days):\n",
    "#     print(df.info())\n",
    "    \n",
    "    # number of datapoints in the data frame\n",
    "    M = len(df.index)\n",
    "    \n",
    "    # creating columns for for each moving average return\n",
    "    col_names = [('MovAvg-' + str(i)) for i in range(2, num_days + 1)]\n",
    "    \n",
    "    for i in range(2, num_days + 1):\n",
    "        # Calculating moving average of the returns of the past i days\n",
    "        data[col_names[i - 2]] = df.rolling(i).mean().shift(1).iloc[i:,]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(num_features, symbol_name = 'AAPL', is_binary_ouput = True):\n",
    "    \"\"\"\n",
    "    INPUT: This function taken in the Symbol of the stock data to be studied and eventually predicted \n",
    "    \n",
    "    It prepares the data for training by fetching the data for the mentioned stock, calculating the daily returns\n",
    "    of the stock, and adding corresponding features for each datapoint using n previous returns. \n",
    "    At the end, if the output is wanted to be binary i.e. Stock going up or down, positive returns are converted \n",
    "    to 1 else converted to 0. It further splits the built dataframe into training and test sets\n",
    "     \n",
    "    RETURN: It returns X and Y values for the train and test sets ready to start work upon\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pp.read_file(symbol_name)\n",
    "    df = pp.data_daily_returns(df, 'ADJ_CLOSE')\n",
    "    \n",
    "    # Removing seasonality\n",
    "    residue = pp.series_decomposition(df, 'additive', 'RETURNS').resid\n",
    "    df = pd.Series.to_frame(residue)\n",
    "    \n",
    "    # creating dataframe to store the LAGGED RETURNS data points\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    data = add_lag_returns(df, data, num_features)\n",
    "#     data = add_moving_average_returns(df, data, num_features)\n",
    "\n",
    "    # Adding the True Return Value of the day to the Dataframe\n",
    "    data['RETURNS'] = df.loc[df.index[num_features:], 'RETURNS']\n",
    "#     print(data.head())\n",
    "\n",
    "    if is_binary_ouput:    \n",
    "        data = output_to_binary_indicators(data)    \n",
    "    \n",
    "    return train_test_split(data, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = prepare_data(5)\n",
    "X_train, X_test, Y_train, Y_test = X_train.values, X_test.values, Y_train.values, Y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn TimeSeriesSplit\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cross_validation(X, Y, num_splits, algorithm, parameters, is_classification = True):\n",
    "    print('Inbuilt Rolling Cross Validation')\n",
    "    if num_splits <= 0 or num_splits >= len(X):\n",
    "        num_splits = len(X) - 1\n",
    "    print('Parameters ------------------------>', parameters)\n",
    "    \n",
    "    accuracies = []\n",
    "    tscv = TimeSeriesSplit(n_splits = num_splits)\n",
    "    \n",
    "    for train, test in tscv.split(X):\n",
    "#         print(\"Test: %s Train: %s\" % (test, train))\n",
    "        X_train, Y_train = X[train], Y[train]\n",
    "        X_test, Y_test = X[test], Y[test]\n",
    "        \n",
    "        Y_pred = algorithm(X_train, Y_train, X_test, parameters[0])\n",
    "        \n",
    "        if is_classification:\n",
    "            accuracies.append(accuracy_score(Y_test, Y_pred))\n",
    "        else:\n",
    "            accuracies.append(r2_score(Y_test, Y_pred))\n",
    "    \n",
    "    mean_accuracy = np.array(accuracies).mean()\n",
    "    print('Accuracy:', mean_accuracy)\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation on a Rolling Basis - Implementation\n",
    "\n",
    "we train our model on a small segment of the time series from the beginning until some  𝑡 , make predictions for the next  𝑡+𝑛 steps, and calculate an error. Then, we expand our training sample to  𝑡+𝑛  value, make predictions from  𝑡+𝑛  until  𝑡+2∗𝑛 , and continue moving our test segment of the time series until we hit the last available observation. As a result, we have as many folds as  𝑛  will fit between the initial training sample and the last observation.  \n",
    "\n",
    "http://francescopochetti.com/pythonic-cross-validation-time-series-pandas-scikit-learn/\n",
    "https://stats.stackexchange.com/a/268847\n",
    "https://robjhyndman.com/hyndsight/tscv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSeriesCV(X_train, Y_train, num_splits, algorithm, is_classification = True):\n",
    "    print('Implemented Rolling Cross Validation')\n",
    "    if num_splits <= 0 or num_splits >= len(X_train):\n",
    "        num_splits = 10\n",
    "    \n",
    "#     print('Number of folds:', num_splits)\n",
    "#     print('Size train set:', X_train.shape)\n",
    "    \n",
    "    # k is the size of each fold. It is computed dividing the number of rows in X_train by the num_splits\n",
    "    # This number is then floored and converted into an int\n",
    "    k = int(np.floor(float(X_train.shape[0]) / num_splits))\n",
    "#     print('Size of each fold:', k)\n",
    "    \n",
    "    # Initialise the accuracies array as an array full of zeroes. It has (num_splits - 1) elements as \n",
    "    # the first element is always used for trainiing and never tested\n",
    "    accuracies = np.zeros(num_splits - 1)\n",
    "    \n",
    "    # Loop from 2 splits to num_splits\n",
    "    for i in range(2, num_splits + 1):\n",
    "        # It is the fraction of the data (used in the iteration) split used for training. Rest is used for testing\n",
    "        split = float(i - 1)/i\n",
    "#         print('Splitting the first ' + str(i) + ' chunks at ' + str(i - 1) + '/' + str(i))\n",
    "        \n",
    "        # Getting the X and Y values to be used in this iteration\n",
    "        X = X_train[:(k * i)]\n",
    "        Y = Y_train[:(k * i)]\n",
    "#         print('Size of train + test:', X.shape)\n",
    "        \n",
    "        # Index to split according to the split fraction calculated earlier\n",
    "        index = int(np.floor(X.shape[0] * split))\n",
    "        \n",
    "        # Folds used to train the model\n",
    "        X_trainFolds = X[:index]\n",
    "        Y_trainFolds = Y[:index]\n",
    "        \n",
    "        # Folds used to test the model\n",
    "        X_testFolds = X[index:]\n",
    "        Y_testFolds = Y[index:]\n",
    "        \n",
    "        algorithm.fit(X_trainFolds, Y_trainFolds)\n",
    "        Y_pred = algorithm.predict(X_testFolds)\n",
    "        \n",
    "        if is_classification:\n",
    "            accuracies[i - 2] = accuracy_score(Y_testFolds, Y_pred)\n",
    "        else:\n",
    "            accuracies[i - 2] = r2_score(Y_testFolds, Y_pred)\n",
    "#         print('Accuracy of fold ' + str(i) + ':' + str(accuracies[i - 2]))\n",
    "#         print('\\n\\n')\n",
    "    \n",
    "    mean_accuracy = accuracies.mean()\n",
    "    print('Accuracy:', mean_accuracy, '\\n')\n",
    "    return mean_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
